{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of this notebook: generate an entry for the kaggle galaxy classification challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] split out a validation set\n",
    "\n",
    "- [x] parse the solution csv file\n",
    "\n",
    "- [x] write a generator to find picture for each solution\n",
    "\n",
    "- [x] use fit_generator to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv, random, os, bcolz, glob\n",
    "import numpy as np\n",
    "from scipy.ndimage import imread\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.pyplot as plt\n",
    "from vgg16 import Vgg16, Dense, Adam, Sequential\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/ubuntu/courses/deeplearning1/nbs'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "LESSON_HOME_DIR = current_dir\n",
    "DATA_HOME_DIR = LESSON_HOME_DIR + \"/data/galaxy\"\n",
    "solutions_csv = DATA_HOME_DIR + \"/solns.csv\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%cd $DATA_HOME_DIR\n",
    "%mkdir valid\n",
    "%mkdir results\n",
    "%mkdir -p sample/train\n",
    "%mkdir -p sample/test\n",
    "%mkdir -p sample/valid\n",
    "%mkdir -p sample/results\n",
    "%mkdir -p test/unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "\n",
    "\n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make validation set"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#MAXSIZE = 2000\n",
    "validation_percent = 0.2\n",
    "\n",
    "train_array = []\n",
    "valid_array = []\n",
    "\n",
    "with open(solutions_csv, 'rb') as csvfile:\n",
    "    solutions_reader = csv.reader(csvfile, delimiter=',')\n",
    "    # skip header\n",
    "    solutions_reader.next()\n",
    "    \n",
    "    ind = 0\n",
    "    for this_soln in solutions_reader:\n",
    "        this_soln = np.array([float(e) for e in this_soln])\n",
    "        if random.random() < validation_percent:\n",
    "            valid_array.append(this_soln)\n",
    "        else:\n",
    "            train_array.append(this_soln)\n",
    "\n",
    "valid_array = np.array(valid_array)\n",
    "train_array = np.array(train_array)\n",
    "\n",
    "print valid_array.shape\n",
    "print train_array.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#move files by id\n",
    "def move_to_validation(uid):\n",
    "    this_file = DATA_HOME_DIR + \"/train/%d.jpg\" % uid\n",
    "    os.rename(this_file, DATA_HOME_DIR + \"/valid/%d.jpg\" % uid)\n",
    "    \n",
    "for valid_img in valid_array:\n",
    "    this_id = int(valid_img[0])\n",
    "    move_to_validation(this_id)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "save_array(DATA_HOME_DIR + \"/valid/valid_solutions\", valid_array)\n",
    "save_array(DATA_HOME_DIR + \"/train/training_solutions\", train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_array = load_array(DATA_HOME_DIR + \"/valid/valid_solutions\")\n",
    "train_array = load_array(DATA_HOME_DIR + \"/train/training_solutions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12159, 38)\n"
     ]
    }
   ],
   "source": [
    "print valid_array.shape\n",
    "#print valid_array[:5, :]\n",
    "#print valid_array[0, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ids = map(int, train_array[:,0])\n",
    "valid_ids = map(int, valid_array[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sample_size = 200\n",
    "valid_sample_size = 100\n",
    "test_sample_size = 200"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%cd $DATA_HOME_DIR/train\n",
    "\n",
    "g = glob.glob(\"*.jpg\")\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(train_sample_size): copyfile(shuf[i], DATA_HOME_DIR+'/sample/train/' + shuf[i])\n",
    "    \n",
    "%cd $DATA_HOME_DIR/valid\n",
    "\n",
    "g = glob.glob(\"*.jpg\")\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(valid_sample_size): copyfile(shuf[i], DATA_HOME_DIR+'/sample/valid/' + shuf[i])\n",
    "    \n",
    "%cd $DATA_HOME_DIR/test\n",
    "\n",
    "g = glob.glob(\"*.jpg\")\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(test_sample_size): copyfile(shuf[i], DATA_HOME_DIR+'/sample/test/' + shuf[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test an idea for generator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "exterior_list = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "def example_gen():\n",
    "    inner_gen = (x for x in exterior_list)\n",
    "    while(1):\n",
    "        try:\n",
    "            yield inner_gen.next()\n",
    "        except StopIteration:\n",
    "            inner_gen = (x for x in exterior_list)\n",
    "            yield inner_gen.next()\n",
    "            \n",
    "try_example = example_gen()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for i in range(10): print(try_example.next())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# now make it batched\n",
    "\n",
    "exterior_list = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "def example_batched_gen(batchsize):\n",
    "    inner_gen = (x for x in exterior_list)\n",
    "    while(1):\n",
    "        output = []\n",
    "        for i in range(batchsize):\n",
    "            try:\n",
    "                output.append(inner_gen.next())\n",
    "            except StopIteration:\n",
    "                inner_gen = (x for x in exterior_list)\n",
    "                output.append(inner_gen.next())\n",
    "        yield output\n",
    "        \n",
    "try_batched_example = example_batched_gen(3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for i in range(10): print(try_batched_example.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make batched generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pic_by_id(uid, folder = DATA_HOME_DIR + \"/train/\"):\n",
    "    return np.swapaxes(np.swapaxes(imresize(imread(folder + \"%d.jpg\" % uid), (224, 224)), 1, 2), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 224, 224)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pic_by_id(train_ids[0]).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_gen = ((get_pic_by_id(rr[0]), rr[1:]) for uid in list(train_array))\n",
    "valid_gen = ((get_pic_by_id(rr[0], DATA_HOME_DIR + \"/valid/\"), rr[1:]) for uid in list(valid_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_gen(solnarray, batchsize, folder = DATA_HOME_DIR + \"/train/\"):\n",
    "    imgen = (get_pic_by_id(rr[0], folder) for rr in list(solnarray))\n",
    "    while(1):\n",
    "        img_out = []\n",
    "        for i in range(batchsize):\n",
    "            try:\n",
    "                this_img = imgen.next()\n",
    "                img_out.append(this_img)\n",
    "            except StopIteration:\n",
    "                imgen = (get_pic_by_id(rr[0], folder) for rr in list(solnarray))\n",
    "                this_img = imgen.next()\n",
    "            \n",
    "        all_img_out = np.stack(img_out, axis=0)\n",
    "        yield all_img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_gen(solnarray, batchsize, folder = DATA_HOME_DIR + \"/train/\"):\n",
    "    imgen = ((get_pic_by_id(rr[0], folder), rr[1:]) for rr in list(solnarray))\n",
    "    while(1):\n",
    "        img_out = []\n",
    "        soln_out = []\n",
    "        for i in range(batchsize):\n",
    "            try:\n",
    "                this_img, this_soln = imgen.next()\n",
    "            except StopIteration:\n",
    "                imgen = ((get_pic_by_id(rr[0], folder), rr[1:]) for rr in list(solnarray))\n",
    "                this_img, this_soln = imgen.next()\n",
    "            img_out.append(this_img)\n",
    "            soln_out.append(this_soln)\n",
    "        all_img_out = np.stack(img_out, axis=0)\n",
    "        all_soln_out = np.stack(soln_out, axis=0)\n",
    "        yield (all_img_out, all_soln_out)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "train_gen = data_gen(train_array, 4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "imgs, solns = train_gen.next()\n",
    "print imgs.shape\n",
    "print solns.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alter the vgg16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "output_size = train_array.shape[1] - 1\n",
    "lr = 0.001\n",
    "\n",
    "model = vgg.model\n",
    "model.pop()\n",
    "for layer in model.layers: layer.trainable = False\n",
    "model.add(Dense(hidden_size, activation = 'relu'))\n",
    "model.add(Dense(output_size, activation = None))\n",
    "model.compile(optimizer=Adam(lr=lr), loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precompute the cnn layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_layers = model.layers[:-2]\n",
    "pre_model = Sequential(pre_layers)\n",
    "#pre_model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "train_features = pre_model.predict_generator(get_batches(DATA_HOME_DIR + \"/train/\", batch_size=64, shuffle=False, class_mode=None), train_array.shape[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "valid_features = pre_model.predict_generator(get_batches(DATA_HOME_DIR + \"/valid/\", batch_size=64, shuffle=False, class_mode=None), valid_array.shape[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "save_array(DATA_HOME_DIR + '/results/train_convlayer_features.bc', trn_features)\n",
    "save_array(DATA_HOME_DIR + '/results/valid_convlayer_features.bc', val_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "49472/49419 [==============================] - 1586s - loss: 0.0627 - val_loss: 0.0276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f08e5a48390>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchsize = 64\n",
    "\n",
    "vgg.model.fit_generator(generator = data_gen(train_array, batchsize),\n",
    "                 samples_per_epoch = train_array.shape[0],\n",
    "                 nb_epoch = 1,\n",
    "                 validation_data = data_gen(valid_array, batchsize, folder = DATA_HOME_DIR + \"/valid/\"),\n",
    "                 nb_val_samples = valid_array.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
