{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of this notebook: generate an entry for the kaggle galaxy classification challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] split out a validation set\n",
    "\n",
    "- [x] parse the solution csv file\n",
    "\n",
    "- [x] write a generator to find picture for each solution\n",
    "\n",
    "- [x] use fit_generator to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv, random, os, bcolz, glob\n",
    "import numpy as np\n",
    "from scipy.ndimage import imread\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.pyplot as plt\n",
    "from vgg16 import Vgg16, Dense, Adam, Sequential\n",
    "from utils import *\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/ubuntu/courses/deeplearning1/nbs'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "LESSON_HOME_DIR = current_dir\n",
    "DATA_HOME_DIR = LESSON_HOME_DIR + \"/data/galaxy\"\n",
    "solutions_csv = DATA_HOME_DIR + \"/solns.csv\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%cd $DATA_HOME_DIR\n",
    "%mkdir valid\n",
    "%mkdir results\n",
    "%mkdir -p sample/train\n",
    "%mkdir -p sample/test\n",
    "%mkdir -p sample/valid\n",
    "%mkdir -p sample/results\n",
    "%mkdir -p test/unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "\n",
    "\n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make validation set"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#MAXSIZE = 2000\n",
    "validation_percent = 0.2\n",
    "\n",
    "train_array = []\n",
    "valid_array = []\n",
    "\n",
    "with open(solutions_csv, 'rb') as csvfile:\n",
    "    solutions_reader = csv.reader(csvfile, delimiter=',')\n",
    "    # skip header\n",
    "    solutions_reader.next()\n",
    "    \n",
    "    ind = 0\n",
    "    for this_soln in solutions_reader:\n",
    "        this_soln = np.array([float(e) for e in this_soln])\n",
    "        if random.random() < validation_percent:\n",
    "            valid_array.append(this_soln)\n",
    "        else:\n",
    "            train_array.append(this_soln)\n",
    "\n",
    "valid_array = np.array(valid_array)\n",
    "train_array = np.array(train_array)\n",
    "\n",
    "print valid_array.shape\n",
    "print train_array.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#move files by id\n",
    "def move_to_validation(uid):\n",
    "    this_file = DATA_HOME_DIR + \"/train/%d.jpg\" % uid\n",
    "    os.rename(this_file, DATA_HOME_DIR + \"/valid/%d.jpg\" % uid)\n",
    "    \n",
    "for valid_img in valid_array:\n",
    "    this_id = int(valid_img[0])\n",
    "    move_to_validation(this_id)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "save_array(DATA_HOME_DIR + \"/valid/valid_solutions\", valid_array)\n",
    "save_array(DATA_HOME_DIR + \"/train/training_solutions\", train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_array = load_array(DATA_HOME_DIR + \"/valid/valid_solutions\")\n",
    "train_array = load_array(DATA_HOME_DIR + \"/train/training_solutions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12159, 38)\n"
     ]
    }
   ],
   "source": [
    "print(valid_array.shape)\n",
    "#print valid_array[:5, :]\n",
    "#print valid_array[0, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ids = map(int, train_array[:,0])\n",
    "valid_ids = map(int, valid_array[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/courses/deeplearning1/nbs/data/galaxy/test\n",
      "[436614, 241401, 204696, 717923, 173724]\n",
      "['436614.jpg', '241401.jpg', '204696.jpg', '717923.jpg', '173724.jpg']\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR/test\n",
    "g = glob(\"*.jpg\")\n",
    "test_ids = [int(filename[:-4]) for filename in g]\n",
    "print(test_ids[:5])\n",
    "print(g[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sample_size = 200\n",
    "valid_sample_size = 100\n",
    "test_sample_size = 200"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%cd $DATA_HOME_DIR/train/unknown\n",
    "\n",
    "g = glob.glob(\"*.jpg\")\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(train_sample_size): copyfile(shuf[i], DATA_HOME_DIR+'/sample/train/' + shuf[i])\n",
    "    \n",
    "%cd $DATA_HOME_DIR/valid/unknown\n",
    "\n",
    "g = glob.glob(\"*.jpg\")\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(valid_sample_size): copyfile(shuf[i], DATA_HOME_DIR+'/sample/valid/' + shuf[i])\n",
    "    \n",
    "%cd $DATA_HOME_DIR/test/unknown\n",
    "\n",
    "g = glob.glob(\"*.jpg\")\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(test_sample_size): copyfile(shuf[i], DATA_HOME_DIR+'/sample/test/' + shuf[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test an idea for generator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "exterior_list = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "def example_gen():\n",
    "    inner_gen = (x for x in exterior_list)\n",
    "    while(1):\n",
    "        try:\n",
    "            yield inner_gen.next()\n",
    "        except StopIteration:\n",
    "            inner_gen = (x for x in exterior_list)\n",
    "            yield inner_gen.next()\n",
    "            \n",
    "try_example = example_gen()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for i in range(10): print(try_example.next())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# now make it batched\n",
    "\n",
    "exterior_list = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "def example_batched_gen(batchsize):\n",
    "    inner_gen = (x for x in exterior_list)\n",
    "    while(1):\n",
    "        output = []\n",
    "        for i in range(batchsize):\n",
    "            try:\n",
    "                output.append(inner_gen.next())\n",
    "            except StopIteration:\n",
    "                inner_gen = (x for x in exterior_list)\n",
    "                output.append(inner_gen.next())\n",
    "        yield output\n",
    "        \n",
    "try_batched_example = example_batched_gen(3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for i in range(10): print(try_batched_example.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make batched generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pic_by_id(uid, folder = DATA_HOME_DIR + \"/train/unknown/\"):\n",
    "    return np.swapaxes(np.swapaxes(imresize(imread(folder + \"%d.jpg\" % uid), (224, 224)), 1, 2), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 224, 224)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pic_by_id(train_ids[0]).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_gen = ((get_pic_by_id(rr[0]), rr[1:]) for uid in list(train_array))\n",
    "valid_gen = ((get_pic_by_id(rr[0], DATA_HOME_DIR + \"/valid/\"), rr[1:]) for uid in list(valid_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict_generator needs a generator that won't continue past the end of the underlying dataset\n",
    "\n",
    "def img_gen(idlist, batchsize, folder = DATA_HOME_DIR + \"/train/unknown/\"):\n",
    "    imgen = (get_pic_by_id(uid, folder) for uid in idlist)\n",
    "    while(1):\n",
    "        img_out = []\n",
    "        for i in range(batchsize):\n",
    "            try:\n",
    "                this_img = imgen.next()\n",
    "                img_out.append(this_img)\n",
    "            except StopIteration:\n",
    "                pass\n",
    "        try:\n",
    "            all_img_out = np.stack(img_out, axis=0)\n",
    "        except ValueError:\n",
    "            raise StopIteration\n",
    "        yield all_img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_gen(solnarray, batchsize, folder = DATA_HOME_DIR + \"/train/unknown/\"):\n",
    "    imgen = ((get_pic_by_id(rr[0], folder), rr[1:]) for rr in list(solnarray))\n",
    "    while(1):\n",
    "        img_out = []\n",
    "        soln_out = []\n",
    "        for i in range(batchsize):\n",
    "            try:\n",
    "                this_img, this_soln = imgen.next()\n",
    "            except StopIteration:\n",
    "                imgen = ((get_pic_by_id(rr[0], folder), rr[1:]) for rr in list(solnarray))\n",
    "                this_img, this_soln = imgen.next()\n",
    "            img_out.append(this_img)\n",
    "            soln_out.append(this_soln)\n",
    "        all_img_out = np.stack(img_out, axis=0)\n",
    "        all_soln_out = np.stack(soln_out, axis=0)\n",
    "        yield (all_img_out, all_soln_out)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "train_gen = data_gen(train_array, 4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "imgs, solns = train_gen.next()\n",
    "print imgs.shape\n",
    "print solns.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alter the vgg16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "output_size = train_array.shape[1] - 1\n",
    "lr = 0.001\n",
    "\n",
    "model = vgg.model\n",
    "model.pop()\n",
    "for layer in model.layers: layer.trainable = False\n",
    "model.add(Dense(hidden_size, activation = 'relu'))\n",
    "model.add(Dense(output_size, activation = None))\n",
    "model.compile(optimizer=Adam(lr=lr), loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precompute the cnn layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49419\n"
     ]
    }
   ],
   "source": [
    "pre_layers = model.layers[:-2]\n",
    "pre_model = Sequential(pre_layers)\n",
    "#pre_model.summary()\n",
    "pred_batch_size = 32\n",
    "num_train_samples = len(train_ids)\n",
    "print(num_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 3, 224, 224)   0           lambda_input_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_1 (ZeroPadding2D)  (None, 3, 226, 226)   0           lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 224, 224)  0           zeropadding2d_1[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_2 (ZeroPadding2D)  (None, 64, 226, 226)  0           convolution2d_1[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 224, 224)  0           zeropadding2d_2[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 112, 112)  0           convolution2d_2[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_3 (ZeroPadding2D)  (None, 64, 114, 114)  0           maxpooling2d_1[5][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 112, 112) 0           zeropadding2d_3[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_4 (ZeroPadding2D)  (None, 128, 114, 114) 0           convolution2d_3[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 128, 112, 112) 0           zeropadding2d_4[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 56, 56)   0           convolution2d_4[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_5 (ZeroPadding2D)  (None, 128, 58, 58)   0           maxpooling2d_2[5][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 256, 56, 56)   0           zeropadding2d_5[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_6 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_5[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 256, 56, 56)   0           zeropadding2d_6[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_7 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_6[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 256, 56, 56)   0           zeropadding2d_7[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 256, 28, 28)   0           convolution2d_7[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_8 (ZeroPadding2D)  (None, 256, 30, 30)   0           maxpooling2d_3[5][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 512, 28, 28)   0           zeropadding2d_8[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_9 (ZeroPadding2D)  (None, 512, 30, 30)   0           convolution2d_8[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 512, 28, 28)   0           zeropadding2d_9[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_10 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_9[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 512, 28, 28)   0           zeropadding2d_10[5][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 512, 14, 14)   0           convolution2d_10[5][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_11 (ZeroPadding2D) (None, 512, 16, 16)   0           maxpooling2d_4[5][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 512, 14, 14)   0           zeropadding2d_11[5][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_12 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_11[5][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 512, 14, 14)   0           zeropadding2d_12[5][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_13 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_12[5][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 512, 14, 14)   0           zeropadding2d_13[5][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 512, 7, 7)     0           convolution2d_13[5][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 25088)         0           maxpooling2d_5[5][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 4096)          0           flatten_1[5][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 4096)          0           dense_1[5][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 4096)          0           dropout_1[5][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 4096)          0           dense_2[5][0]                    \n",
      "====================================================================================================\n",
      "Total params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pre_model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pred_gen = img_gen(train_ids, pred_batch_size)\n",
    "\n",
    "num_batches = num_train_samples / pred_batch_size + 1\n",
    "all_preds = []\n",
    "this_batch_num = 1\n",
    "while(1):\n",
    "    try:\n",
    "        all_preds.append(pre_model.predict_on_batch(pred_gen.next()))\n",
    "        print('\\r', \"Batch \",this_batch_num, \" of \", num_batches, end='')\n",
    "    except StopIteration:\n",
    "        break\n",
    "    this_batch_num += 1\n",
    "train_features = np.vstack(all_preds)\n",
    "print(\"\\n\", train_features.shape)\n",
    "#train_features = pre_model.predict_generator(pred_gen, train_sample_size / pred_batch_size) #train_array.shape[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "save_array(DATA_HOME_DIR + '/results/train_convlayer_features.bc', train_features)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_valid_samples = len(valid_ids)\n",
    "valid_pred_gen = img_gen(valid_ids,\n",
    "                         pred_batch_size,\n",
    "                        folder = DATA_HOME_DIR + \"/valid/unknown/\")\n",
    "\n",
    "num_batches = num_valid_samples / pred_batch_size + 1\n",
    "all_preds = []\n",
    "this_batch_num = 1\n",
    "while(1):\n",
    "    try:\n",
    "        all_preds.append(pre_model.predict_on_batch(valid_pred_gen.next()))\n",
    "        print('\\r', \"Batch \",this_batch_num, \" of \", num_batches, end='')\n",
    "    except StopIteration:\n",
    "        break\n",
    "    this_batch_num += 1\n",
    "valid_features = np.vstack(all_preds)\n",
    "print(\"\\n\", valid_features.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "save_array(DATA_HOME_DIR + '/results/valid_convlayer_features.bc', valid_features)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_test_samples = len(test_ids)\n",
    "test_pred_gen = img_gen(test_ids,\n",
    "                         pred_batch_size,\n",
    "                        folder = DATA_HOME_DIR + \"/test/\")\n",
    "\n",
    "num_batches = num_test_samples / pred_batch_size + 1\n",
    "all_preds = []\n",
    "this_batch_num = 1\n",
    "while(1):\n",
    "    try:\n",
    "        all_preds.append(pre_model.predict_on_batch(test_pred_gen.next()))\n",
    "        print('\\r', \"Batch \",this_batch_num, \" of \", num_batches, end='')\n",
    "    except StopIteration:\n",
    "        break\n",
    "    this_batch_num += 1\n",
    "test_features = np.vstack(all_preds)\n",
    "print(\"\\n\", test_features.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "save_array(DATA_HOME_DIR + '/results/test_convlayer_features.bc', test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_features = load_array(DATA_HOME_DIR + '/results/valid_convlayer_features.bc')\n",
    "train_features = load_array(DATA_HOME_DIR + '/results/train_convlayer_features.bc')\n",
    "test_features = load_array(DATA_HOME_DIR + '/results/test_convlayer_features.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up just the last few layers to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden1_size = 128\n",
    "hidden2_size = 128\n",
    "output_size = 37\n",
    "\n",
    "fc_model = Sequential([\n",
    "        Dense(hidden1_size, input_dim=4096, activation='relu'),\n",
    "        Dense(hidden2_size, activation='relu'),\n",
    "        Dense(output_size, activation=None)\n",
    "    ])\n",
    "\n",
    "fc_model.compile(optimizer = RMSprop(lr=0.001, rho=0.7),\n",
    "                loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49419, 37)\n"
     ]
    }
   ],
   "source": [
    "train_solns = train_array[:, 1:]\n",
    "valid_solns = valid_array[:, 1:]\n",
    "print(train_solns.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49419 samples, validate on 12159 samples\n",
      "Epoch 1/5\n",
      "49419/49419 [==============================] - 3s - loss: 0.0298 - val_loss: 0.0198\n",
      "Epoch 2/5\n",
      "49419/49419 [==============================] - 3s - loss: 0.0190 - val_loss: 0.0206\n",
      "Epoch 3/5\n",
      "49419/49419 [==============================] - 3s - loss: 0.0179 - val_loss: 0.0215\n",
      "Epoch 4/5\n",
      "49419/49419 [==============================] - 3s - loss: 0.0173 - val_loss: 0.0179\n",
      "Epoch 5/5\n",
      "49419/49419 [==============================] - 3s - loss: 0.0169 - val_loss: 0.0208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f89cd57ee10>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_batchsize = 64\n",
    "\n",
    "fc_model.fit(x = train_features,\n",
    "             y = train_solns,\n",
    "             validation_data = (valid_features, valid_solns),\n",
    "             batch_size = fit_batchsize,\n",
    "             nb_epoch = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_model.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49419 samples, validate on 12159 samples\n",
      "Epoch 1/5\n",
      "49419/49419 [==============================] - 3s - loss: 0.0153 - val_loss: 0.0179\n",
      "Epoch 2/5\n",
      "49419/49419 [==============================] - 3s - loss: 0.0153 - val_loss: 0.0173\n",
      "Epoch 3/5\n",
      "49419/49419 [==============================] - 3s - loss: 0.0152 - val_loss: 0.0181\n",
      "Epoch 4/5\n",
      "49419/49419 [==============================] - 3s - loss: 0.0152 - val_loss: 0.0166\n",
      "Epoch 5/5\n",
      "49419/49419 [==============================] - 3s - loss: 0.0151 - val_loss: 0.0163\n"
     ]
    }
   ],
   "source": [
    "this_hist = fc_model.fit(x = train_features,\n",
    "             y = train_solns,\n",
    "             validation_data = (valid_features, valid_solns),\n",
    "             batch_size = fit_batchsize,\n",
    "             nb_epoch = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.127835715089\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(this_hist.history[\"val_loss\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49419 samples, validate on 12159 samples\n",
      "Epoch 1/5\n",
      "49419/49419 [==============================] - 4s - loss: 0.0269 - val_loss: 0.0271\n",
      "Epoch 2/5\n",
      "49419/49419 [==============================] - 4s - loss: 0.0269 - val_loss: 0.0270\n",
      "Epoch 3/5\n",
      "49419/49419 [==============================] - 4s - loss: 0.0269 - val_loss: 0.0270\n",
      "Epoch 4/5\n",
      "49419/49419 [==============================] - 4s - loss: 0.0269 - val_loss: 0.0271\n",
      "Epoch 5/5\n",
      "49419/49419 [==============================] - 4s - loss: 0.0269 - val_loss: 0.0270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8a0d74eed0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.fit(x = train_features,\n",
    "             y = train_solns,\n",
    "             validation_data = (valid_features, valid_solns),\n",
    "             batch_size = fit_batchsize,\n",
    "             nb_epoch = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79975, 37)\n"
     ]
    }
   ],
   "source": [
    "test_predicts = fc_model.predict(test_features, pred_batch_size)\n",
    "print(test_predicts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79975, 38)\n"
     ]
    }
   ],
   "source": [
    "kaggle_predicts = np.hstack([np.reshape(np.array(test_ids), (-1, 1)), test_predicts])\n",
    "print(kaggle_predicts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%d,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f\n"
     ]
    }
   ],
   "source": [
    "fmt_str = '%d,'+'%.5f,'*37\n",
    "fmt_str = fmt_str[:-1]\n",
    "print(fmt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GalaxyID,Class1.1,Class1.2,Class1.3,Class2.1,Class2.2,Class3.1,Class3.2,Class4.1,Class4.2,Class5.1,Class5.2,Class5.3,Class5.4,Class6.1,Class6.2,Class7.1,Class7.2,Class7.3,Class8.1,Class8.2,Class8.3,Class8.4,Class8.5,Class8.6,Class8.7,Class9.1,Class9.2,Class9.3,Class10.1,Class10.2,Class10.3,Class11.1,Class11.2,Class11.3,Class11.4,Class11.5,Class11.6\n"
     ]
    }
   ],
   "source": [
    "with open(solutions_csv, 'rb') as csvfile:\n",
    "    solutions_reader = csv.reader(csvfile, delimiter=',')\n",
    "    # skip header\n",
    "    solns_header = \",\".join(solutions_reader.next())\n",
    "    \n",
    "print(solns_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/courses/deeplearning1/nbs/data/galaxy\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR\n",
    "submission_file_name = 'galaxy_submission_2.csv'\n",
    "np.savetxt(submission_file_name, kaggle_predicts, fmt=fmt_str, header=solns_header, comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/courses/deeplearning1/nbs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='data/galaxy/galaxy_submission_2.csv' target='_blank'>data/galaxy/galaxy_submission_2.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/courses/deeplearning1/nbs/data/galaxy/galaxy_submission_2.csv"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "%cd $LESSON_HOME_DIR\n",
    "FileLink(\"data/galaxy/\" + submission_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
